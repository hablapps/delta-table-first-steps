{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.circe.parser._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time.Instant\n",
       "\n",
       "\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`io.delta::delta-core:0.5.0`\n",
    "import $ivy.`io.circe::circe-parser:0.12.3`\n",
    "import $ivy.`com.lihaoyi::os-lib:0.2.7`\n",
    "import org.apache.spark.sql._\n",
    "import io.circe.parser._\n",
    "import java.time.Instant\n",
    "\n",
    "os.remove.all(os.pwd / \"parquet-example\")\n",
    "os.remove.all(os.pwd / \"parquet-example2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Working with simple parquet files\n",
    "\n",
    "Parquet it's a great data file format, and in combination with spark, makes a powerfull combination to make our data analisis faster and cheaper. I'm not gonna explain all that makes parquet so good, but let's see in what we can do in a ordinary day with it.\n",
    "\n",
    "But first, let's put the tools that we are going to use:\n",
    "\n",
    "* First, apache spark is one of the most popular big data processing, if you want to learn more, contact us for information of our trainings in [habla computing](www.habla.dev) or consultancy.\n",
    "* In this notebook, we are going to work in a local instance of spark, so all the data that we generate will be in the local file system. To make it easier to see the generated files, we are going to use [OS lib](https://github.com/lihaoyi/os-lib) a simple, flexible, high-performance Scala interface to common OS filesystem and subprocess APIs. The only bad thing is that is not compatible with hadoop file system, but for the porpouse of this notebook, it's enough.\n",
    "* Also some files that we will create, will be in [json](https://www.json.org/json-en.html) format, to show them properly we will use [circe](https://circe.github.io/circe/) another simple library, to parse, manipulate and validate json documents.\n",
    "* And the last thing, is the kernel used in this notebook, it's called [almond](https://almond.sh/) and its a jupiter kernel based in [ammmonite]() a Scala REPL. It allows to configure everithing, and import all the dependencies as a normal build tool.\n",
    "\n",
    "So you came for delta tables, and you leave in adition with the basics of two fantastic libraries in scala, and a powerful scala kernel for jupyter ðŸ˜„.\n",
    "\n",
    "One more thing, the notebooks are tested to be reprocessed as many times as you want (and need), so if you find any error, just start executing from the begining.\n",
    "\n",
    "Ok let's star with some code, first lets create the spark session and write a dataframe in a parquet file, so we can explain how to read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/04/13 12:37:42 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/04/13 12:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/04/13 12:37:43 INFO SparkContext: Submitted application: parquet-test\n",
      "20/04/13 12:37:43 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/04/13 12:37:43 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/04/13 12:37:43 INFO SecurityManager: Changing view acls groups to: \n",
      "20/04/13 12:37:43 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/04/13 12:37:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/04/13 12:37:43 INFO Utils: Successfully started service 'sparkDriver' on port 40549.\n",
      "20/04/13 12:37:43 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/04/13 12:37:43 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/04/13 12:37:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/04/13 12:37:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/04/13 12:37:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ea68bdb2-ff29-41b6-a3aa-86419e7916aa\n",
      "20/04/13 12:37:43 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/04/13 12:37:43 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/04/13 12:37:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/04/13 12:37:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://de658133f556:4040\n",
      "20/04/13 12:37:44 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/04/13 12:37:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39963.\n",
      "20/04/13 12:37:44 INFO NettyBlockTransferService: Server created on de658133f556:39963\n",
      "20/04/13 12:37:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/04/13 12:37:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, de658133f556, 39963, None)\n",
      "20/04/13 12:37:44 INFO BlockManagerMasterEndpoint: Registering block manager de658133f556:39963 with 366.3 MB RAM, BlockManagerId(driver, de658133f556, 39963, None)\n",
      "20/04/13 12:37:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, de658133f556, 39963, None)\n",
      "20/04/13 12:37:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, de658133f556, 39963, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@21ec5f0a"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"parquet-test\").master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\") // this is to log only the essencial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"parquet-example\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"parquet-example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).write.format(\"parquet\").save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, all set up, we have a parquet file, so we can read it in a very simple way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mreadedFile\u001b[39m: \u001b[32mDataFrame\u001b[39m = [id: bigint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val readedFile = spark.read.parquet(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the dataframe of the file, already knows the structure of the data, this is one of the first interesting things in parquet, it writes the metadata in it's own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "readedFile.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could do anything that we want wit the dataframe, but we are going to focus more in the read and write of the files, so lets learn how to write the file, it is as simple as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mnewfile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"parquet-example2\"\u001b[39m"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val newfile = \"parquet-example2\"\n",
    "readedFile.write.parquet(newfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we have someting in the folder, for this we are going to use the OS lib, create a reference to the destiny and list it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/parquet-example2/._SUCCESS.crc\n",
      "/home/jovyan/work/parquet-example2/.part-00000-41892a33-685c-4f26-b941-c40bb459ca60-c000.snappy.parquet.crc\n",
      "/home/jovyan/work/parquet-example2/_SUCCESS\n",
      "/home/jovyan/work/parquet-example2/part-00000-41892a33-685c-4f26-b941-c40bb459ca60-c000.snappy.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mwd\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mpwd\u001b[39m.\u001b[32mThisType\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"parquet-example2\"\u001b[39m"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wd = os.pwd / newfile //pwd is the working folder\n",
    "os.list(wd).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, the file that we created, it's actualy a folder, and spark puts all the neaded files bellow it.\n",
    "\n",
    "But imagine that we found that we have an error, and want to write the data again, let's see what happend using the same line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31morg.apache.spark.sql.AnalysisException: path file:/home/jovyan/work/parquet-example2 already exists.;\u001b[39m\n  org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(\u001b[32mInsertIntoHadoopFsRelationCommand.scala\u001b[39m:\u001b[32m114\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m104\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(\u001b[32mcommands.scala\u001b[39m:\u001b[32m102\u001b[39m)\n  org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(\u001b[32mcommands.scala\u001b[39m:\u001b[32m122\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m131\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m155\u001b[39m)\n  org.apache.spark.rdd.RDDOperationScope$.withScope(\u001b[32mRDDOperationScope.scala\u001b[39m:\u001b[32m151\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.executeQuery(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m152\u001b[39m)\n  org.apache.spark.sql.execution.SparkPlan.execute(\u001b[32mSparkPlan.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m83\u001b[39m)\n  org.apache.spark.sql.execution.QueryExecution.toRdd(\u001b[32mQueryExecution.scala\u001b[39m:\u001b[32m81\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.$anonfun$runCommand$1(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m80\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m127\u001b[39m)\n  org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(\u001b[32mSQLExecution.scala\u001b[39m:\u001b[32m75\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.runCommand(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m676\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.saveToV1Source(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m290\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m271\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.save(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m229\u001b[39m)\n  org.apache.spark.sql.DataFrameWriter.parquet(\u001b[32mDataFrameWriter.scala\u001b[39m:\u001b[32m566\u001b[39m)\n  ammonite.$sess.cmd9$Helper.<init>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m1\u001b[39m)\n  ammonite.$sess.cmd9$.<init>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m7\u001b[39m)\n  ammonite.$sess.cmd9$.<clinit>(\u001b[32mcmd9.sc\u001b[39m:\u001b[32m-1\u001b[39m)"
     ]
    }
   ],
   "source": [
    "readedFile.write.parquet(newfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oukey... run time error. This is because spark by default use the save mode \"ErrorIfExist\" that if it already find a that the folder exist and contains data, it throws an error.\n",
    "\n",
    "In our case we want to use the save mode \"Overwrite\" to delete all the previous data stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "readedFile.write.mode(SaveMode.Overwrite).parquet(newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/parquet-example2/._SUCCESS.crc\n",
      "/home/jovyan/work/parquet-example2/.part-00000-21a86f2b-aae4-43d8-b722-b1cf5dbdeacc-c000.snappy.parquet.crc\n",
      "/home/jovyan/work/parquet-example2/_SUCCESS\n",
      "/home/jovyan/work/parquet-example2/part-00000-21a86f2b-aae4-43d8-b722-b1cf5dbdeacc-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All cleaned and only we can find the last data stored, as we wanted.\n",
    "\n",
    "This is a good way to kickstart a new parquet table, but usualy, we don't erase all the data and put the new one, what we do is to add the new data to the previous table, or more commonly called `Append`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(newfile).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(10,15).write.mode(SaveMode.Append).parquet(newfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(newfile).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice, it added all the new data that we provided.\n",
    "\n",
    "But at this moment, we are going to talk about limitations. All kinds of files in big data have the same problems:\n",
    "* Lack of history. As you show, when we ovewrite the file, it deleted all the previous data, and as a developer, it's very common to mistake the save mode that we wanted to use, and delete a production table by mistake ðŸ˜°.\n",
    "* Upsert some data it's not easy, if we want to modify some of the records of our table, and keep them in the same table, we will have to write the results first in an auxiliary table, and then overwrite the original one. This is because is never a good practice to overwrite a table that it's been readed in the same process. Also, as you can imagine, moving the data of the extenal table, and the new one, means that the table can't be accessed for reading because it's in maintenance.\n",
    "\n",
    "This problems are realated to the lack of [ACID properties](https://en.wikipedia.org/wiki/ACID) in big data files or tables.\n",
    "\n",
    "But [Databricks](https://databricks.com/) has open sourced a new file format called [delta lake](https://delta.io), that uses parquet as the persistence format, and solves a lot of the problems that we find in our everyday life.\n",
    "\n",
    "Go to the [next notebook](./DeltaTableExample.ipynb) to learn about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
