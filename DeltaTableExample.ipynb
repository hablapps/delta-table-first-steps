{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta tables\n",
    "\n",
    "Spark has become a standard way to process data in a distributed way. It's simplicity and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.circe.parser._\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`io.delta::delta-core:0.5.0`\n",
    "import $ivy.`io.circe::circe-parser:0.12.3`\n",
    "import $ivy.`com.lihaoyi::os-lib:0.2.7`\n",
    "import org.apache.spark.sql._\n",
    "import io.circe.parser._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"delta-example\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"delta-example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwd\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mpwd\u001b[39m.\u001b[32mThisType\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wd = os.pwd / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove.all(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/03/19 12:05:07 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/03/19 12:05:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/03/19 12:05:08 INFO SparkContext: Submitted application: delta-test\n",
      "20/03/19 12:05:08 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/03/19 12:05:08 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/03/19 12:05:08 INFO SecurityManager: Changing view acls groups to: \n",
      "20/03/19 12:05:08 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/03/19 12:05:08 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/03/19 12:05:09 INFO Utils: Successfully started service 'sparkDriver' on port 41643.\n",
      "20/03/19 12:05:09 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/03/19 12:05:09 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/03/19 12:05:09 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/03/19 12:05:09 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/03/19 12:05:09 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-281d5256-803c-4f60-8049-f1e5ca83009d\n",
      "20/03/19 12:05:09 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/03/19 12:05:09 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/03/19 12:05:10 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/03/19 12:05:10 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://35d6bfa155e0:4040\n",
      "20/03/19 12:05:10 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/03/19 12:05:10 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43327.\n",
      "20/03/19 12:05:10 INFO NettyBlockTransferService: Server created on 35d6bfa155e0:43327\n",
      "20/03/19 12:05:10 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/03/19 12:05:10 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 35d6bfa155e0, 43327, None)\n",
      "20/03/19 12:05:10 INFO BlockManagerMasterEndpoint: Registering block manager 35d6bfa155e0:43327 with 366.3 MB RAM, BlockManagerId(driver, 35d6bfa155e0, 43327, None)\n",
      "20/03/19 12:05:10 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 35d6bfa155e0, 43327, None)\n",
      "20/03/19 12:05:10 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 35d6bfa155e0, 43327, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3b338fe6"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"delta-test\").master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Version 0\n",
    "\n",
    "Now that we have our spark session, we want to try this delta format, so we are going to write some data to it. The only diference to write parquet, is that the format is \"delta\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).write.format(\"delta\").save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure, lets show what we have written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "| 25|\n",
      "| 26|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look of the folder were we wrote. We will se that we have written a single parquet file, due to the single core of our standalone cluster. But also, it created a folder called `_delta_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/.part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet.crc\n",
      "/home/jovyan/work/delta-example/_delta_log\n",
      "/home/jovyan/work/delta-example/part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the files, we see that it contais a single json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000000.json\n"
     ]
    }
   ],
   "source": [
    "val logFolder = wd / \"_delta_log\"\n",
    "os.list(logFolder).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use [circe](https://circe.github.io/circe/) to see the content of this file in a pretty way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion0\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000000.json\"\u001b[39m\n",
       "\u001b[36mversion0Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[32m\"\"\"{\n",
       "  \"commitInfo\" : {\n",
       "    \"timestamp\" : 1584619529321,\n",
       "\u001b[39m..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version0 = os.list(logFolder).find(_.last.endsWith(\"0.json\")).get\n",
    "val version0Json = os.read.lines(version0).map(x => parse(x).right.get.spaces2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the file is a json document.The first one, has the commit infomation, showing us the timestamp of the commit, the type of operation, with the parameters of it, and also if it was a bling append, an append that didn't take previous commits in account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1584619529321,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"ErrorIfExists\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"isBlindAppend\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second document, contains information of the protocol used for delta table, with the minimun version of reader and writer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"protocol\" : {\n",
      "    \"minReaderVersion\" : 1,\n",
      "    \"minWriterVersion\" : 2\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third, contains metadata information, with the schema, the format of the files used, in this case parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metaData\" : {\n",
      "    \"id\" : \"1eb697db-a282-44dc-8877-8c481ba8adc6\",\n",
      "    \"format\" : {\n",
      "      \"provider\" : \"parquet\",\n",
      "      \"options\" : {\n",
      "        \n",
      "      }\n",
      "    },\n",
      "    \"schemaString\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "    \"partitionColumns\" : [\n",
      "    ],\n",
      "    \"configuration\" : {\n",
      "      \n",
      "    },\n",
      "    \"createdTime\" : 1584619527565\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last, shows that it has added a new file with the path of the new parquet file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 483,\n",
      "    \"modificationTime\" : 1584619529000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's append a new bunch of data, and see how has changed the folder. And look what contains in it now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "| 25|\n",
      "| 26|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10,20).write.format(\"delta\").mode(SaveMode.Append).save(file)\n",
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing unexpected, it mixed the previous data with the new one. If we check the parquet files, we will haver our previous file, and a new one, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/part-00000-afa7f105-90b3-4381-9241-870fd63d889e-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).filter(_.last.endsWith(\"parquet\")).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the log? Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000000.json\n",
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "os.list(logFolder).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a new file that ends in 0001.json, this is the latest version of the log, let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion1\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000001.json\"\u001b[39m\n",
       "\u001b[36mversion1Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mio\u001b[39m.\u001b[32mcirce\u001b[39m.\u001b[32mJson\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[33mJObject\u001b[39m(\n",
       "    object[commitInfo -> {\n",
       "  \"timestamp\" : 1584619539174,\n",
       "..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version1 = os.list(logFolder).find(_.last.endsWith(\"1.json\")).get\n",
    "val version1Json = os.read.lines(version1).map(x => parse(x).right.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "This `version 1` file only has two lines, one like the previous one with the information of the commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1584619539174,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"Append\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"readVersion\" : 0,\n",
      "    \"isBlindAppend\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version1Json(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second one, also like the [version 0](#Version-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-afa7f105-90b3-4381-9241-870fd63d889e-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 472,\n",
      "    \"modificationTime\" : 1584619539000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version1Json(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won't see any difference between delta table and workning with parquet at this point.\n",
    "\n",
    "But lets overwrite our data, and we will see the first real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(20,30).write.format(\"delta\").mode(SaveMode.Overwrite).save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the overwrite was done ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "| 25|\n",
      "| 26|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we list our parquet files, we will see that not only it has added a new one, it also mantains the previous that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/part-00000-33246b79-752e-4818-af09-c25065f23bf8-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-afa7f105-90b3-4381-9241-870fd63d889e-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).filter(_.last.endsWith(\"parquet\")).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? The anser is in the new log file created, this time with index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion2\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000002.json\"\u001b[39m\n",
       "\u001b[36mversion2Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[32m\"\"\"{\n",
       "  \"commitInfo\" : {\n",
       "    \"timestamp\" : 1584619875932,\n",
       "\u001b[39m..."
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version2 = os.list(logFolder).find(_.last.endsWith(\"2.json\")).get\n",
    "val version2Json = os.read.lines(version2).map(x => parse(x).right.get.spaces2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1584619875932,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"Overwrite\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"readVersion\" : 1,\n",
      "    \"isBlindAppend\" : false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version2Json(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-33246b79-752e-4818-af09-c25065f23bf8-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 472,\n",
      "    \"modificationTime\" : 1584619873000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"remove\" : {\n",
      "    \"path\" : \"part-00000-d33b3f10-5015-4161-a781-78a49dd12f1f-c000.snappy.parquet\",\n",
      "    \"deletionTimestamp\" : 1584619875931,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"remove\" : {\n",
      "    \"path\" : \"part-00000-afa7f105-90b3-4381-9241-870fd63d889e-c000.snappy.parquet\",\n",
      "    \"deletionTimestamp\" : 1584619875932,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version2Json(1))\n",
    "println(version2Json(2))\n",
    "println(version2Json(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the log is indicating that its adding a new parquet file (line with \"add\" key), but also, that in this version, it must not take in account the previous files(lines with the \"remove\" key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can uderstand that what delta table is doing. It isn't resolving the actions in the folder like parquet that would have aresed all previous data. What delta is doing it's keeping all the data files, in this case parquet files, and logging the changes in the '_delta_log' throw versioning.\n",
    "\n",
    "This way, delta can recreate in any moment no only the last version, as we would do in a normal parquet file, also previous state versions.\n",
    "\n",
    "So if we can recreate them, how we can read the file, but like the version 0 or 1? indicating in the options the version that we want to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(file).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(file).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can use the option \"timestampAsOf\" to get the last version available at that moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion1Timestamp\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1584619539174L\u001b[39m"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version1Timestamp = version1Json(0).hcursor.downField(\"commitInfo\").get[Long](\"timestamp\").right.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
