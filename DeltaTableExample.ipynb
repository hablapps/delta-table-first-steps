{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "repl.pprinter() = repl.pprinter().copy(defaultHeight = 5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delta tables\n",
    "\n",
    "Spark SQL has become a standard way to process data in a distributed way. The dataframe and dataset api allow us to do all kinds of transformations with our data.\n",
    "\n",
    "Normaly, when we speak about file formats in big data, we mostly use parquet. As we show in the [previous notebook](./Working%20with%20parquet.ipynb), parquet is a columnar file format created for big data precesess. The amount of metadata, and columnar way to storage our data, combined with a great sinergy with Apache spark allows us to try to read only the esencial data. But for this format as we show, and for all formats in big data, we have a problem to administrate this files in a file system point of view. When we need to do a mantainance in this tables, due to cleaning, or GDPR actions, we allways work more or less in the following way.\n",
    "First, stop everithing related with this file, nobody can read it.\n",
    "Second, launch the process and write the result in original folder.\n",
    "\n",
    "Let's start, but first, let's prepare the enviroment, with a reference `wd` to the folder that we will persist the data, create the spark session and we will be ready to rock ðŸ¤˜."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                           \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                              \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                          \n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mio.circe.parser._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjava.time.Instant\u001b[39m"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.5`\n",
    "import $ivy.`io.delta::delta-core:0.5.0`\n",
    "import $ivy.`io.circe::circe-parser:0.12.3`\n",
    "import $ivy.`com.lihaoyi::os-lib:0.2.7`\n",
    "import org.apache.spark.sql._\n",
    "import io.circe.parser._\n",
    "import java.time.Instant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfile\u001b[39m: \u001b[32mString\u001b[39m = \u001b[32m\"delta-example\"\u001b[39m"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val file = \"delta-example\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mwd\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mpackage\u001b[39m.\u001b[32mpwd\u001b[39m.\u001b[32mThisType\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wd = os.pwd / file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove.all(wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "20/04/13 13:09:25 INFO SparkContext: Running Spark version 2.4.5\n",
      "20/04/13 13:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "20/04/13 13:09:25 INFO SparkContext: Submitted application: delta-test\n",
      "20/04/13 13:09:25 INFO SecurityManager: Changing view acls to: jovyan\n",
      "20/04/13 13:09:25 INFO SecurityManager: Changing modify acls to: jovyan\n",
      "20/04/13 13:09:25 INFO SecurityManager: Changing view acls groups to: \n",
      "20/04/13 13:09:25 INFO SecurityManager: Changing modify acls groups to: \n",
      "20/04/13 13:09:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(jovyan); groups with view permissions: Set(); users  with modify permissions: Set(jovyan); groups with modify permissions: Set()\n",
      "20/04/13 13:09:26 INFO Utils: Successfully started service 'sparkDriver' on port 37215.\n",
      "20/04/13 13:09:26 INFO SparkEnv: Registering MapOutputTracker\n",
      "20/04/13 13:09:26 INFO SparkEnv: Registering BlockManagerMaster\n",
      "20/04/13 13:09:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "20/04/13 13:09:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "20/04/13 13:09:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b18d1ccb-f9aa-463b-9286-01af201aca44\n",
      "20/04/13 13:09:26 INFO MemoryStore: MemoryStore started with capacity 366.3 MB\n",
      "20/04/13 13:09:26 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "20/04/13 13:09:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "20/04/13 13:09:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://de658133f556:4040\n",
      "20/04/13 13:09:27 INFO Executor: Starting executor ID driver on host localhost\n",
      "20/04/13 13:09:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39211.\n",
      "20/04/13 13:09:27 INFO NettyBlockTransferService: Server created on de658133f556:39211\n",
      "20/04/13 13:09:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "20/04/13 13:09:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, de658133f556, 39211, None)\n",
      "20/04/13 13:09:28 INFO BlockManagerMasterEndpoint: Registering block manager de658133f556:39211 with 366.3 MB RAM, BlockManagerId(driver, de658133f556, 39211, None)\n",
      "20/04/13 13:09:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, de658133f556, 39211, None)\n",
      "20/04/13 13:09:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, de658133f556, 39211, None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@612c2819"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val spark = SparkSession.builder().appName(\"delta-test\").master(\"local[1]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 0\n",
    "\n",
    "Now that we have our spark session, we want to try this delta format, so we are going to write some data to it. The only diference to write parquet, is that the format is \"delta\". (Remember, by default the save mode is \"Error if exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(1,10).write.format(\"delta\").save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be sure, lets show what we have written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look of the folder were we wrote. We will se that we have written a single parquet file, due to the single core of our standalone cluster. But also, it created a folder called `_delta_log`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/.part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet.crc\n",
      "/home/jovyan/work/delta-example/_delta_log\n",
      "/home/jovyan/work/delta-example/part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listing the files, we see that it contais a json file inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000000.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[36mlogFolder\u001b[39m: \u001b[32mwd\u001b[39m.\u001b[32mThisType\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val logFolder = wd / \"_delta_log\"\n",
    "os.list(logFolder).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see the main difference with parquet. Each time we write something in our delta table, we will have a commit log in this folder. But to learn about it, let's deep dive in them.\n",
    "\n",
    "We can use [circe](https://circe.github.io/circe/) to see the content of this file in a pretty way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion0\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000000.json\"\u001b[39m\n",
       "\u001b[36mversion0Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[32m\"\"\"{\n",
       "  \"commitInfo\" : {\n",
       "    \"timestamp\" : 1586783385009,\n",
       "\u001b[39m..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version0 = os.list(logFolder).find(_.last.endsWith(\"0.json\")).get\n",
    "val version0Json = os.read.lines(version0).map(x => parse(x).right.get.spaces2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each line of the file is a json document.The first one, has the commit infomation, showing us the timestamp of the commit, the type of operation, with the parameters of it, and also if it was a bling append, an append that didn't take previous commits in account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1586783385009,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"ErrorIfExists\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"isBlindAppend\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second document, contains information of the protocol used for delta table, with the minimun version of reader and writer needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"protocol\" : {\n",
      "    \"minReaderVersion\" : 1,\n",
      "    \"minWriterVersion\" : 2\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third, contains metadata information, with the schema, the format of the files used, in this case parquet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"metaData\" : {\n",
      "    \"id\" : \"65c12125-d7a9-4270-9c1d-8bee6f83644c\",\n",
      "    \"format\" : {\n",
      "      \"provider\" : \"parquet\",\n",
      "      \"options\" : {\n",
      "        \n",
      "      }\n",
      "    },\n",
      "    \"schemaString\" : \"{\\\"type\\\":\\\"struct\\\",\\\"fields\\\":[{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"long\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}]}\",\n",
      "    \"partitionColumns\" : [\n",
      "    ],\n",
      "    \"configuration\" : {\n",
      "      \n",
      "    },\n",
      "    \"createdTime\" : 1586783383392\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the last, shows that it has added a new file with the path of the new parquet file created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 483,\n",
      "    \"modificationTime\" : 1586783384000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version0Json(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version1\n",
    "\n",
    "Let's append a new bunch of data, and take a look of what it contains now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.range(10,20).write.format(\"delta\").mode(SaveMode.Append).save(file)\n",
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing unexpected, it mixed the previous data with the new one. If we check the parquet files, we will haver our previous file, and a new one, as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/part-00000-27a15051-cebb-42d0-93fd-8fa7914b964d-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).filter(_.last.endsWith(\"parquet\")).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in the log? Take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000000.json\n",
      "/home/jovyan/work/delta-example/_delta_log/00000000000000000001.json\n"
     ]
    }
   ],
   "source": [
    "os.list(logFolder).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is a new file that ends in 0001.json, this is the latest version of the log, let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion1\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000001.json\"\u001b[39m\n",
       "\u001b[36mversion1Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mio\u001b[39m.\u001b[32mcirce\u001b[39m.\u001b[32mJson\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[33mJObject\u001b[39m(\n",
       "    object[commitInfo -> {\n",
       "  \"timestamp\" : 1586783397056,\n",
       "..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version1 = os.list(logFolder).find(_.last.endsWith(\"1.json\")).get\n",
    "val version1Json = os.read.lines(version1).map(x => parse(x).right.get)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This `version 1` file only has two lines, one like the previous one with the information of the commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1586783397056,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"Append\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"readVersion\" : 0,\n",
      "    \"isBlindAppend\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version1Json(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the second one, also like the [version 0](#Version-0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-27a15051-cebb-42d0-93fd-8fa7914b964d-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 472,\n",
      "    \"modificationTime\" : 1586783397000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version1Json(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You won't see any difference between delta table and workning with parquet at this point.\n",
    "\n",
    "## Version2\n",
    "\n",
    "But lets overwrite our data, and we will see the first real difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.range(20,30).write.format(\"delta\").mode(SaveMode.Overwrite).save(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that the overwrite was done ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "| 20|\n",
      "| 21|\n",
      "| 22|\n",
      "| 23|\n",
      "| 24|\n",
      "| 25|\n",
      "| 26|\n",
      "| 27|\n",
      "| 28|\n",
      "| 29|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").load(file).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we list our parquet files, we will see that not only it has added a new one, it also mantains the previous that we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/delta-example/part-00000-27a15051-cebb-42d0-93fd-8fa7914b964d-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet\n",
      "/home/jovyan/work/delta-example/part-00000-300ce4ec-52b7-423e-8e0e-285503ed66bd-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "os.list(wd).filter(_.last.endsWith(\"parquet\")).foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? The anser is in the new log file created, this time with index 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion2\u001b[39m: \u001b[32mos\u001b[39m.\u001b[32mPath\u001b[39m = root/\u001b[32m'home\u001b[39m/\u001b[32m'jovyan\u001b[39m/\u001b[32m'work\u001b[39m/\u001b[32m\"delta-example\"\u001b[39m/\u001b[32m'_delta_log\u001b[39m/\u001b[32m\"00000000000000000002.json\"\u001b[39m\n",
       "\u001b[36mversion2Json\u001b[39m: \u001b[32mcollection\u001b[39m.\u001b[32mmutable\u001b[39m.\u001b[32mWrappedArray\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mWrappedArray\u001b[39m(\n",
       "  \u001b[32m\"\"\"{\n",
       "  \"commitInfo\" : {\n",
       "    \"timestamp\" : 1586783407642,\n",
       "\u001b[39m..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version2 = os.list(logFolder).find(_.last.endsWith(\"2.json\")).get\n",
    "val version2Json = os.read.lines(version2).map(x => parse(x).right.get.spaces2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"commitInfo\" : {\n",
      "    \"timestamp\" : 1586783407642,\n",
      "    \"operation\" : \"WRITE\",\n",
      "    \"operationParameters\" : {\n",
      "      \"mode\" : \"Overwrite\",\n",
      "      \"partitionBy\" : \"[]\"\n",
      "    },\n",
      "    \"readVersion\" : 1,\n",
      "    \"isBlindAppend\" : false\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version2Json(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"add\" : {\n",
      "    \"path\" : \"part-00000-300ce4ec-52b7-423e-8e0e-285503ed66bd-c000.snappy.parquet\",\n",
      "    \"partitionValues\" : {\n",
      "      \n",
      "    },\n",
      "    \"size\" : 472,\n",
      "    \"modificationTime\" : 1586783406000,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"remove\" : {\n",
      "    \"path\" : \"part-00000-2a1a7ab1-2f5e-4530-bb70-436ba04cc095-c000.snappy.parquet\",\n",
      "    \"deletionTimestamp\" : 1586783407641,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n",
      "{\n",
      "  \"remove\" : {\n",
      "    \"path\" : \"part-00000-27a15051-cebb-42d0-93fd-8fa7914b964d-c000.snappy.parquet\",\n",
      "    \"deletionTimestamp\" : 1586783407641,\n",
      "    \"dataChange\" : true\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "println(version2Json(1))\n",
    "println(version2Json(2))\n",
    "println(version2Json(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the log is indicating that its adding a new parquet file (line with \"add\" key), but also, that in this version, it must not take in account the previous files(lines with the \"remove\" key)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can uderstand that what delta table is doing. It isn't resolving the actions in the folder like parquet that would have eresed all previous data. What delta is doing it's keeping all the data files, in this case parquet files, and logging the changes in the '_delta_log' throw versioning.\n",
    "\n",
    "This way, delta can recreate in any moment no only the last version, as we would do in a normal parquet file, also previous state versions.\n",
    "\n",
    "So if we can recreate them, how we can read the file, but like the version 0 or 1? indicating in the options the version that we want to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(file).show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(file).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, we can use the option \"timestampAsOf\" to get the last version available at that moment. For this we can take advance of the optics implementation in circe JSON object to obtain the timestamp of the version 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mversion1Timestamp\u001b[39m: \u001b[32mLong\u001b[39m = \u001b[32m1586783397056L\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val version1Timestamp = version1Json(0).hcursor.downField(\"commitInfo\").get[Long](\"timestamp\").right.get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And create two timestamps, one slightly earlier to the version 1 and other later to version 1. The format of the date must be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mpreviousToVersion1\u001b[39m: \u001b[32mInstant\u001b[39m = 2020-04-13T13:09:52.056Z\n",
       "\u001b[36mlaterToVersion1\u001b[39m: \u001b[32mInstant\u001b[39m = 2020-04-13T13:10:02.056Z"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val previousToVersion1 = Instant.ofEpochMilli(version1Timestamp - 5000)\n",
    "val laterToVersion1 = Instant.ofEpochMilli(version1Timestamp + 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we use a timestamp previous to version 1 (and later thant version 0 of course), we will recover all the data in that moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"timestampAsOf\", previousToVersion1.toString).load(file).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the same for the later than version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "| 10|\n",
      "| 11|\n",
      "| 12|\n",
      "| 13|\n",
      "| 14|\n",
      "| 15|\n",
      "| 16|\n",
      "| 17|\n",
      "| 18|\n",
      "| 19|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.format(\"delta\").option(\"timestampAsOf\", laterToVersion1.toString).load(file).show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if it doesn't overwrite the data, could we read and delete from the same delta table? Let's try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
